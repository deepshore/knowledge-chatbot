{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic llama index usage\n",
    "\n",
    "- Install / Setup\n",
    "- Load Documents\n",
    "- Create & Store Index\n",
    "- Query Index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install libraries\n",
    "\n",
    "- This section will install the necessary libraries for the dataloaders and index implementations.\n",
    "\n",
    "### what is llama-hub ?\n",
    "\n",
    "llama-hub is a library/framework for llm applications with a weight on data integrations.\n",
    "it uses langchain (another well-known library) and the openai drivers under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama_index\n",
    "!pip install llama_hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup\n",
    "\n",
    "- set logging to stream to stdout and loglevel to info\n",
    "- set openai api key\n",
    "\n",
    "### why openai api key?\n",
    "\n",
    "llama-hub and many other llm app-frameworks or libraries use (per default) openai high-level api for embeddings (the interaction with the model).\n",
    "\n",
    "![](images/chatbot_graph-white-bg-v3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import getpass\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# using this to connect to openai\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "#os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data from website via sitemap\n",
    "\n",
    "### what is a sitemap?\n",
    "\n",
    "A sitemap is a file where you provide information about the pages, videos, and other files on your site, and the relationships between them. \n",
    "\n",
    "Learn more:\n",
    "https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview\n",
    "\n",
    "### why use a sitemap in this case?\n",
    "\n",
    "Many websites have interesting content and already offer a sitemap.xml. So I can just choose my input by choosing a website of my interest.\n",
    "This is how I can create a custom pool of knowledge for my chatbot.\n",
    "\n",
    "### how do we get the data?\n",
    "\n",
    "using the [llama_hub loader for sitemaps](https://llama-hub-ui.vercel.app/l/web-sitemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.web.sitemap.base import SitemapReader\n",
    "\n",
    "# for jupyter notebooks uncomment the following two lines of code:\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "loader = SitemapReader(html_to_text=True)\n",
    "documents = loader.load_data(sitemap_url='https://deepshore.de/sitemap.xml', filter='https://deepshore.de/knowledge')\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index from Documents\n",
    "\n",
    "### why\n",
    "a vector index stores/indexes vector embeddings for fast retrieval and similarity search.\n",
    "\n",
    "![](images/how-does-vector-store-work.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load index from from disk storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query index\n",
    "\n",
    "simply query the index and look at the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Was ist k6.io? Wof√ºr benutzt man es?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
